# File: run-all-once.yml
# Code: Claude Code and Codex
# Review: Ryoichi Ando (ryoichi.ando@zozo.com)
# License: Apache v2.0
# Generated by: run-all-once-gen.py

name: All Examples

on:
  workflow_dispatch:
    inputs:
      instance_type:
        description: 'EC2 instance type'
        required: true
        default: 'g6e.2xlarge'
        type: choice
        options:
          - g6.2xlarge
          - g6e.2xlarge
      region:
        description: 'AWS Region'
        required: true
        default: 'us-east-2'
        type: choice
        options:
          - us-east-1
          - us-east-2
          - ap-northeast-1

jobs:
  run-batch-1:
    name: Run Batch 1
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read

    env:
      AWS_REGION: ${{ github.event.inputs.region }}
      INSTANCE_TYPE: ${{ github.event.inputs.instance_type }}
      BRANCH: ${{ github.ref_name }}
      EXAMPLES: "belt cards codim curtain domino"
      WORKDIR: /home/ubuntu
      USER: ubuntu

    steps:
      - name: Show input parameters
        run: |
          echo "## Input Parameters - Batch 1"
          echo "Branch: ${{ github.ref_name }}"
          echo "Instance Type: ${{ github.event.inputs.instance_type }}"
          echo "Region: ${{ github.event.inputs.region }}"
          echo "Examples: belt cards codim curtain domino"

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-duration-seconds: 21600

      - name: Find Deep Learning AMI and network resources
        id: setup
        run: |
          echo "Finding latest Deep Learning AMI with GPU support..."
          AMI_ID=$(aws ec2 describe-images \
            --owners amazon \
            --filters \
              "Name=name,Values=Deep Learning Base OSS Nvidia Driver GPU AMI (Ubuntu 24.04)*" \
              "Name=state,Values=available" \
              "Name=architecture,Values=x86_64" \
            --query 'sort_by(Images, &CreationDate)[-1].ImageId' \
            --region "$AWS_REGION" \
            --output text)

          if [ "$AMI_ID" = "None" ] || [ -z "$AMI_ID" ]; then
            echo "ERROR: Deep Learning AMI not found in region $AWS_REGION"
            exit 1
          fi
          echo "AMI_ID=$AMI_ID" >> $GITHUB_OUTPUT
          echo "Found AMI: $AMI_ID"

          # Get GitHub Actions dedicated VPC, subnet, and security group
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=github-actions-vpc" --query 'Vpcs[0].VpcId' --output text)
          SUBNET_ID=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[0].SubnetId' --output text)
          SG_ID=$(aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$VPC_ID" "Name=group-name,Values=github-actions-sg" --query 'SecurityGroups[0].GroupId' --output text)

          if [ "$VPC_ID" = "None" ] || [ -z "$VPC_ID" ]; then
            echo "ERROR: github-actions-vpc not found in region $AWS_REGION"
            exit 1
          fi

          echo "::add-mask::$VPC_ID"
          echo "::add-mask::$SUBNET_ID"
          echo "::add-mask::$SG_ID"
          echo "SUBNET_ID=$SUBNET_ID" >> $GITHUB_OUTPUT
          echo "SG_ID=$SG_ID" >> $GITHUB_OUTPUT
          echo "VPC: $VPC_ID, Subnet: $SUBNET_ID, SG: $SG_ID"

      - name: Generate unique identifiers and SSH key
        id: ids
        run: |
          TIMESTAMP=$(date +%Y%m%d%H%M%S)
          RANDOM_SUFFIX=$(head /dev/urandom | tr -dc a-z0-9 | head -c 6)
          TEMP_INSTANCE_ID="temp-${TIMESTAMP}-${RANDOM_SUFFIX}"
          echo "TIMESTAMP=$TIMESTAMP" >> $GITHUB_OUTPUT
          echo "TEMP_INSTANCE_ID=$TEMP_INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "Temporary Instance ID: $TEMP_INSTANCE_ID"

          # Generate SSH key early so we can embed it in user-data
          rm -f /tmp/ec2key /tmp/ec2key.pub
          ssh-keygen -t rsa -f /tmp/ec2key -N "" -q
          echo "SSH key generated"

      - name: Create user data script
        run: |
          SSH_PUBKEY=$(cat /tmp/ec2key.pub)
          cat > /tmp/user-data.sh << EOF
          #!/bin/bash
          set -x
          exec > >(tee /var/log/user-data.log) 2>&1

          echo "=== User Data Script Started ==="

          # Wait for system to be ready
          sleep 5

          # Setup SSH key for persistent authentication (no 60s expiry)
          mkdir -p /home/ubuntu/.ssh
          echo "${SSH_PUBKEY}" >> /home/ubuntu/.ssh/authorized_keys
          chown -R ubuntu:ubuntu /home/ubuntu/.ssh
          chmod 700 /home/ubuntu/.ssh
          chmod 600 /home/ubuntu/.ssh/authorized_keys
          echo "SSH key installed permanently"

          # Verify nvidia-smi is available
          if command -v nvidia-smi &> /dev/null; then
              echo "NVIDIA drivers confirmed"
              nvidia-smi
          else
              echo "Warning: nvidia-smi not found"
          fi

          # Create workspace directory
          mkdir -p /home/ubuntu/workspace
          chown -R ubuntu:ubuntu /home/ubuntu/workspace

          nvidia-smi | tee /tmp/nvidia-smi-output.txt
          touch /tmp/setup-complete
          echo "=== User Data Script Complete ==="
          EOF

      - name: Launch EC2 instance
        id: instance
        run: |
          echo "Launching EC2 instance..."

          # Base64 encode for AWS
          USER_DATA=$(base64 -w 0 /tmp/user-data.sh)

          INSTANCE_ID=$(aws ec2 run-instances \
            --image-id "${{ steps.setup.outputs.AMI_ID }}" \
            --instance-type "$INSTANCE_TYPE" \
            --subnet-id "${{ steps.setup.outputs.SUBNET_ID }}" \
            --security-group-ids "${{ steps.setup.outputs.SG_ID }}" \
            --associate-public-ip-address \
            --user-data "$USER_DATA" \
            --block-device-mappings "DeviceName=/dev/sda1,Ebs={VolumeSize=256,VolumeType=gp3,DeleteOnTermination=true}" \
            --tag-specifications \
              "ResourceType=instance,Tags=[\
                {Key=Name,Value=gpu-runner-batch-1-${{ steps.ids.outputs.TIMESTAMP }}},\
                {Key=ManagedBy,Value=GitHubActions},\
                {Key=Purpose,Value=GPURunner},\
                {Key=Workflow,Value=${{ github.workflow }}},\
                {Key=RunId,Value=${{ github.run_id }}},\
                {Key=Branch,Value=${{ env.BRANCH }}},\
                {Key=Batch,Value=1}\
              ]" \
              "ResourceType=volume,Tags=[\
                {Key=Name,Value=gpu-runner-batch-1-${{ steps.ids.outputs.TIMESTAMP }}-volume},\
                {Key=ManagedBy,Value=GitHubActions},\
                {Key=Purpose,Value=GPURunner},\
                {Key=Batch,Value=1}\
              ]" \
            --instance-initiated-shutdown-behavior terminate \
            --query 'Instances[0].InstanceId' \
            --region "$AWS_REGION" \
            --output text)

          echo "INSTANCE_ID=$INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "$INSTANCE_ID" > /tmp/instance_id.txt
          echo "Instance launched: $INSTANCE_ID"

      - name: Wait for instance and user-data
        run: |
          INSTANCE_ID=$(cat /tmp/instance_id.txt)
          echo "Waiting for instance to be running..."
          aws ec2 wait instance-running --instance-ids "$INSTANCE_ID" --region "$AWS_REGION"

          echo "Instance is running, waiting for user-data to complete..."

          # Wait for setup-complete (user-data finished, SSH key installed)
          MAX_ATTEMPTS=60
          for i in $(seq 1 $MAX_ATTEMPTS); do
            # Open tunnel temporarily
            aws ec2-instance-connect open-tunnel \
              --instance-id "$INSTANCE_ID" \
              --local-port 2222 &
            TUNNEL_PID=$!
            sleep 3

            # Check if setup is complete (includes SSH connectivity check)
            if ssh -i /tmp/ec2key -p 2222 \
              -o StrictHostKeyChecking=no \
              -o UserKnownHostsFile=/dev/null \
              -o ConnectTimeout=5 \
              ubuntu@localhost "test -f /tmp/setup-complete && echo READY" 2>/dev/null | grep -q READY; then
              echo "Instance setup completed on attempt $i"
              kill $TUNNEL_PID 2>/dev/null || true
              break
            fi

            kill $TUNNEL_PID 2>/dev/null || true

            if [ $i -eq $MAX_ATTEMPTS ]; then
              echo "Setup timeout after $MAX_ATTEMPTS attempts, continuing anyway..."
              break
            fi

            echo "Attempt $i/$MAX_ATTEMPTS: Setup not complete, waiting 10s..."
            sleep 10
          done

      - name: Create archive of repository
        run: |
          echo "Creating repository archive..."
          git archive --format=tar.gz --output=/tmp/repo.tar.gz HEAD

      - name: Transfer repository to instance
        run: |
          echo "Transferring repository to instance..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          rsync -avz -e "ssh -i /tmp/ec2key -p 2222 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
            /tmp/repo.tar.gz ubuntu@localhost:${{env.WORKDIR}}/

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            ubuntu@localhost \
            "cd ${{env.WORKDIR}} && tar -xzf repo.tar.gz && rm repo.tar.gz"

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Setup Python environment and run warmup
        run: |
          echo "Setting up Python environment and running warmup.py..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -e
          cd ${{env.WORKDIR}}

          # Run warmup.py
          echo "Running warmup.py..."
          python3 warmup.py --skip-confirmation

          echo "Warmup completed"
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Build Rust project
        run: |
          echo "Building Rust project with cargo..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -e
          cd ${{env.WORKDIR}}

          # Setup Rust environment
          source "$HOME/.cargo/env"

          # Build the project
          echo "Running cargo build --release..."
          cargo build --release

          echo "Cargo build completed"
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Setup CI directory
        run: |
          INSTANCE_ID=$(cat /tmp/instance_id.txt)
          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5
          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            ubuntu@localhost "mkdir -p /tmp/ci"
          kill $TUNNEL_PID 2>/dev/null || true

      - name: Run belt
        run: |
          echo "Running belt..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -eo pipefail
          cd ${{env.WORKDIR}}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/belt.ipynb" --output "/tmp/belt_base.py"

          # Create the runnable script with proper imports (using printf to avoid nested heredoc)
          printf '%s\n' 'import sys' 'import os' '' '# Add the repository root to Python path' "sys.path.insert(0, '${{env.WORKDIR}}')" "sys.path.insert(0, '${{env.WORKDIR}}/frontend')" '' '# Set environment variables' "os.environ['"'PYTHONPATH'"'] = '${{env.WORKDIR}}:${{env.WORKDIR}}/frontend:' + os.environ.get('"'PYTHONPATH'"', '')" > /tmp/belt.py

          # Append the converted notebook content
          cat "/tmp/belt_base.py" >> /tmp/belt.py

          # Create output directory for this example
          mkdir -p /tmp/ci/belt

          # Run the example
          echo "belt" > frontend/.CI
          python3 /tmp/belt.py 2>&1 | tee /tmp/ci/belt/belt.log
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Run cards
        run: |
          echo "Running cards..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -eo pipefail
          cd ${{env.WORKDIR}}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/cards.ipynb" --output "/tmp/cards_base.py"

          # Create the runnable script with proper imports (using printf to avoid nested heredoc)
          printf '%s\n' 'import sys' 'import os' '' '# Add the repository root to Python path' "sys.path.insert(0, '${{env.WORKDIR}}')" "sys.path.insert(0, '${{env.WORKDIR}}/frontend')" '' '# Set environment variables' "os.environ['"'PYTHONPATH'"'] = '${{env.WORKDIR}}:${{env.WORKDIR}}/frontend:' + os.environ.get('"'PYTHONPATH'"', '')" > /tmp/cards.py

          # Append the converted notebook content
          cat "/tmp/cards_base.py" >> /tmp/cards.py

          # Create output directory for this example
          mkdir -p /tmp/ci/cards

          # Run the example
          echo "cards" > frontend/.CI
          python3 /tmp/cards.py 2>&1 | tee /tmp/ci/cards/cards.log
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Run codim
        run: |
          echo "Running codim..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -eo pipefail
          cd ${{env.WORKDIR}}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/codim.ipynb" --output "/tmp/codim_base.py"

          # Create the runnable script with proper imports (using printf to avoid nested heredoc)
          printf '%s\n' 'import sys' 'import os' '' '# Add the repository root to Python path' "sys.path.insert(0, '${{env.WORKDIR}}')" "sys.path.insert(0, '${{env.WORKDIR}}/frontend')" '' '# Set environment variables' "os.environ['"'PYTHONPATH'"'] = '${{env.WORKDIR}}:${{env.WORKDIR}}/frontend:' + os.environ.get('"'PYTHONPATH'"', '')" > /tmp/codim.py

          # Append the converted notebook content
          cat "/tmp/codim_base.py" >> /tmp/codim.py

          # Create output directory for this example
          mkdir -p /tmp/ci/codim

          # Run the example
          echo "codim" > frontend/.CI
          python3 /tmp/codim.py 2>&1 | tee /tmp/ci/codim/codim.log
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Run curtain
        run: |
          echo "Running curtain..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -eo pipefail
          cd ${{env.WORKDIR}}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/curtain.ipynb" --output "/tmp/curtain_base.py"

          # Create the runnable script with proper imports (using printf to avoid nested heredoc)
          printf '%s\n' 'import sys' 'import os' '' '# Add the repository root to Python path' "sys.path.insert(0, '${{env.WORKDIR}}')" "sys.path.insert(0, '${{env.WORKDIR}}/frontend')" '' '# Set environment variables' "os.environ['"'PYTHONPATH'"'] = '${{env.WORKDIR}}:${{env.WORKDIR}}/frontend:' + os.environ.get('"'PYTHONPATH'"', '')" > /tmp/curtain.py

          # Append the converted notebook content
          cat "/tmp/curtain_base.py" >> /tmp/curtain.py

          # Create output directory for this example
          mkdir -p /tmp/ci/curtain

          # Run the example
          echo "curtain" > frontend/.CI
          python3 /tmp/curtain.py 2>&1 | tee /tmp/ci/curtain/curtain.log
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Run domino
        run: |
          echo "Running domino..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -eo pipefail
          cd ${{env.WORKDIR}}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/domino.ipynb" --output "/tmp/domino_base.py"

          # Create the runnable script with proper imports (using printf to avoid nested heredoc)
          printf '%s\n' 'import sys' 'import os' '' '# Add the repository root to Python path' "sys.path.insert(0, '${{env.WORKDIR}}')" "sys.path.insert(0, '${{env.WORKDIR}}/frontend')" '' '# Set environment variables' "os.environ['"'PYTHONPATH'"'] = '${{env.WORKDIR}}:${{env.WORKDIR}}/frontend:' + os.environ.get('"'PYTHONPATH'"', '')" > /tmp/domino.py

          # Append the converted notebook content
          cat "/tmp/domino_base.py" >> /tmp/domino.py

          # Create output directory for this example
          mkdir -p /tmp/ci/domino

          # Run the example
          echo "domino" > frontend/.CI
          python3 /tmp/domino.py 2>&1 | tee /tmp/ci/domino/domino.log
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true


      - name: Collect results
        if: success() || failure()
        run: |
          echo "Collecting results from all runs..."
          mkdir -p ci

          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          # Open tunnel for this step
          aws ec2-instance-connect open-tunnel \
            --instance-id "$INSTANCE_ID" \
            --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          # Delete large binary files on remote before copying to save bandwidth
          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            ubuntu@localhost \
            "find ~/.cache/ppf-cts/ci -type f \( -name '*.bin' -o -name '*.pickle' -o -name '*.ply' -o -name '*.gz' \) -delete 2>/dev/null" || true

          # Copy CI output from ppf-cts cache directory
          rsync -avz -e "ssh -i /tmp/ec2key -p 2222 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
            ubuntu@localhost:~/.cache/ppf-cts/ci/ ./ci/ || echo "No ppf-cts CI files found"

          # Also copy logs from /tmp/ci
          rsync -avz -e "ssh -i /tmp/ec2key -p 2222 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
            ubuntu@localhost:/tmp/ci/ ./ci/ || echo "No log files found"

          echo "## Collected Files:"
          ls -laR ci/ | head -100

          # Close tunnel
          kill $TUNNEL_PID 2>/dev/null || true

      - name: Upload artifact
        if: success() || failure()
        uses: actions/upload-artifact@v4
        with:
          name: ci-batch-1
          path: ci
          retention-days: 3

      - name: GPU information
        if: success() || failure()
        run: |
          echo "Getting GPU information..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          # Open tunnel for this step
          aws ec2-instance-connect open-tunnel \
            --instance-id "$INSTANCE_ID" \
            --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            ubuntu@localhost "nvidia-smi" || echo "Failed to get GPU info"

          # Close tunnel
          kill $TUNNEL_PID 2>/dev/null || true

      - name: Re-authenticate for cleanup
        if: always()
        continue-on-error: true
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-duration-seconds: 21600

      - name: Cleanup - Terminate Instance
        if: always()
        continue-on-error: true
        run: |
          if [ -n "${{ steps.instance.outputs.INSTANCE_ID }}" ]; then
            echo "Initiating instance termination: ${{ steps.instance.outputs.INSTANCE_ID }}"
            aws ec2 terminate-instances \
              --instance-ids "${{ steps.instance.outputs.INSTANCE_ID }}" \
              --region "$AWS_REGION" || true
            echo "Termination initiated. Instance will terminate in the background."
          else
            echo "No instance to terminate"
          fi

      - name: Summary
        if: always()
        run: |
          echo "## Workflow Summary - Batch 1"
          echo "- Region: $AWS_REGION"
          echo "- Instance Type: $INSTANCE_TYPE"
          echo "- Branch: $BRANCH"
          echo "- Examples: $EXAMPLES"
          echo "- Instance ID: ${{ steps.instance.outputs.INSTANCE_ID || 'Not launched' }}"

  run-batch-2:
    name: Run Batch 2
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read

    env:
      AWS_REGION: ${{ github.event.inputs.region }}
      INSTANCE_TYPE: ${{ github.event.inputs.instance_type }}
      BRANCH: ${{ github.ref_name }}
      EXAMPLES: "drape fishingknot fitting friction"
      WORKDIR: /home/ubuntu
      USER: ubuntu

    steps:
      - name: Show input parameters
        run: |
          echo "## Input Parameters - Batch 2"
          echo "Branch: ${{ github.ref_name }}"
          echo "Instance Type: ${{ github.event.inputs.instance_type }}"
          echo "Region: ${{ github.event.inputs.region }}"
          echo "Examples: drape fishingknot fitting friction"

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-duration-seconds: 21600

      - name: Find Deep Learning AMI and network resources
        id: setup
        run: |
          echo "Finding latest Deep Learning AMI with GPU support..."
          AMI_ID=$(aws ec2 describe-images \
            --owners amazon \
            --filters \
              "Name=name,Values=Deep Learning Base OSS Nvidia Driver GPU AMI (Ubuntu 24.04)*" \
              "Name=state,Values=available" \
              "Name=architecture,Values=x86_64" \
            --query 'sort_by(Images, &CreationDate)[-1].ImageId' \
            --region "$AWS_REGION" \
            --output text)

          if [ "$AMI_ID" = "None" ] || [ -z "$AMI_ID" ]; then
            echo "ERROR: Deep Learning AMI not found in region $AWS_REGION"
            exit 1
          fi
          echo "AMI_ID=$AMI_ID" >> $GITHUB_OUTPUT
          echo "Found AMI: $AMI_ID"

          # Get GitHub Actions dedicated VPC, subnet, and security group
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=github-actions-vpc" --query 'Vpcs[0].VpcId' --output text)
          SUBNET_ID=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[0].SubnetId' --output text)
          SG_ID=$(aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$VPC_ID" "Name=group-name,Values=github-actions-sg" --query 'SecurityGroups[0].GroupId' --output text)

          if [ "$VPC_ID" = "None" ] || [ -z "$VPC_ID" ]; then
            echo "ERROR: github-actions-vpc not found in region $AWS_REGION"
            exit 1
          fi

          echo "::add-mask::$VPC_ID"
          echo "::add-mask::$SUBNET_ID"
          echo "::add-mask::$SG_ID"
          echo "SUBNET_ID=$SUBNET_ID" >> $GITHUB_OUTPUT
          echo "SG_ID=$SG_ID" >> $GITHUB_OUTPUT
          echo "VPC: $VPC_ID, Subnet: $SUBNET_ID, SG: $SG_ID"

      - name: Generate unique identifiers and SSH key
        id: ids
        run: |
          TIMESTAMP=$(date +%Y%m%d%H%M%S)
          RANDOM_SUFFIX=$(head /dev/urandom | tr -dc a-z0-9 | head -c 6)
          TEMP_INSTANCE_ID="temp-${TIMESTAMP}-${RANDOM_SUFFIX}"
          echo "TIMESTAMP=$TIMESTAMP" >> $GITHUB_OUTPUT
          echo "TEMP_INSTANCE_ID=$TEMP_INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "Temporary Instance ID: $TEMP_INSTANCE_ID"

          # Generate SSH key early so we can embed it in user-data
          rm -f /tmp/ec2key /tmp/ec2key.pub
          ssh-keygen -t rsa -f /tmp/ec2key -N "" -q
          echo "SSH key generated"

      - name: Create user data script
        run: |
          SSH_PUBKEY=$(cat /tmp/ec2key.pub)
          cat > /tmp/user-data.sh << EOF
          #!/bin/bash
          set -x
          exec > >(tee /var/log/user-data.log) 2>&1

          echo "=== User Data Script Started ==="

          # Wait for system to be ready
          sleep 5

          # Setup SSH key for persistent authentication (no 60s expiry)
          mkdir -p /home/ubuntu/.ssh
          echo "${SSH_PUBKEY}" >> /home/ubuntu/.ssh/authorized_keys
          chown -R ubuntu:ubuntu /home/ubuntu/.ssh
          chmod 700 /home/ubuntu/.ssh
          chmod 600 /home/ubuntu/.ssh/authorized_keys
          echo "SSH key installed permanently"

          # Verify nvidia-smi is available
          if command -v nvidia-smi &> /dev/null; then
              echo "NVIDIA drivers confirmed"
              nvidia-smi
          else
              echo "Warning: nvidia-smi not found"
          fi

          # Create workspace directory
          mkdir -p /home/ubuntu/workspace
          chown -R ubuntu:ubuntu /home/ubuntu/workspace

          nvidia-smi | tee /tmp/nvidia-smi-output.txt
          touch /tmp/setup-complete
          echo "=== User Data Script Complete ==="
          EOF

      - name: Launch EC2 instance
        id: instance
        run: |
          echo "Launching EC2 instance..."

          # Base64 encode for AWS
          USER_DATA=$(base64 -w 0 /tmp/user-data.sh)

          INSTANCE_ID=$(aws ec2 run-instances \
            --image-id "${{ steps.setup.outputs.AMI_ID }}" \
            --instance-type "$INSTANCE_TYPE" \
            --subnet-id "${{ steps.setup.outputs.SUBNET_ID }}" \
            --security-group-ids "${{ steps.setup.outputs.SG_ID }}" \
            --associate-public-ip-address \
            --user-data "$USER_DATA" \
            --block-device-mappings "DeviceName=/dev/sda1,Ebs={VolumeSize=256,VolumeType=gp3,DeleteOnTermination=true}" \
            --tag-specifications \
              "ResourceType=instance,Tags=[\
                {Key=Name,Value=gpu-runner-batch-2-${{ steps.ids.outputs.TIMESTAMP }}},\
                {Key=ManagedBy,Value=GitHubActions},\
                {Key=Purpose,Value=GPURunner},\
                {Key=Workflow,Value=${{ github.workflow }}},\
                {Key=RunId,Value=${{ github.run_id }}},\
                {Key=Branch,Value=${{ env.BRANCH }}},\
                {Key=Batch,Value=2}\
              ]" \
              "ResourceType=volume,Tags=[\
                {Key=Name,Value=gpu-runner-batch-2-${{ steps.ids.outputs.TIMESTAMP }}-volume},\
                {Key=ManagedBy,Value=GitHubActions},\
                {Key=Purpose,Value=GPURunner},\
                {Key=Batch,Value=2}\
              ]" \
            --instance-initiated-shutdown-behavior terminate \
            --query 'Instances[0].InstanceId' \
            --region "$AWS_REGION" \
            --output text)

          echo "INSTANCE_ID=$INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "$INSTANCE_ID" > /tmp/instance_id.txt
          echo "Instance launched: $INSTANCE_ID"

      - name: Wait for instance and user-data
        run: |
          INSTANCE_ID=$(cat /tmp/instance_id.txt)
          echo "Waiting for instance to be running..."
          aws ec2 wait instance-running --instance-ids "$INSTANCE_ID" --region "$AWS_REGION"

          echo "Instance is running, waiting for user-data to complete..."

          # Wait for setup-complete (user-data finished, SSH key installed)
          MAX_ATTEMPTS=60
          for i in $(seq 1 $MAX_ATTEMPTS); do
            # Open tunnel temporarily
            aws ec2-instance-connect open-tunnel \
              --instance-id "$INSTANCE_ID" \
              --local-port 2222 &
            TUNNEL_PID=$!
            sleep 3

            # Check if setup is complete (includes SSH connectivity check)
            if ssh -i /tmp/ec2key -p 2222 \
              -o StrictHostKeyChecking=no \
              -o UserKnownHostsFile=/dev/null \
              -o ConnectTimeout=5 \
              ubuntu@localhost "test -f /tmp/setup-complete && echo READY" 2>/dev/null | grep -q READY; then
              echo "Instance setup completed on attempt $i"
              kill $TUNNEL_PID 2>/dev/null || true
              break
            fi

            kill $TUNNEL_PID 2>/dev/null || true

            if [ $i -eq $MAX_ATTEMPTS ]; then
              echo "Setup timeout after $MAX_ATTEMPTS attempts, continuing anyway..."
              break
            fi

            echo "Attempt $i/$MAX_ATTEMPTS: Setup not complete, waiting 10s..."
            sleep 10
          done

      - name: Create archive of repository
        run: |
          echo "Creating repository archive..."
          git archive --format=tar.gz --output=/tmp/repo.tar.gz HEAD

      - name: Transfer repository to instance
        run: |
          echo "Transferring repository to instance..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          rsync -avz -e "ssh -i /tmp/ec2key -p 2222 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
            /tmp/repo.tar.gz ubuntu@localhost:${{env.WORKDIR}}/

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            ubuntu@localhost \
            "cd ${{env.WORKDIR}} && tar -xzf repo.tar.gz && rm repo.tar.gz"

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Setup Python environment and run warmup
        run: |
          echo "Setting up Python environment and running warmup.py..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -e
          cd ${{env.WORKDIR}}

          # Run warmup.py
          echo "Running warmup.py..."
          python3 warmup.py --skip-confirmation

          echo "Warmup completed"
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Build Rust project
        run: |
          echo "Building Rust project with cargo..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -e
          cd ${{env.WORKDIR}}

          # Setup Rust environment
          source "$HOME/.cargo/env"

          # Build the project
          echo "Running cargo build --release..."
          cargo build --release

          echo "Cargo build completed"
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Setup CI directory
        run: |
          INSTANCE_ID=$(cat /tmp/instance_id.txt)
          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5
          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            ubuntu@localhost "mkdir -p /tmp/ci"
          kill $TUNNEL_PID 2>/dev/null || true

      - name: Run drape
        run: |
          echo "Running drape..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -eo pipefail
          cd ${{env.WORKDIR}}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/drape.ipynb" --output "/tmp/drape_base.py"

          # Create the runnable script with proper imports (using printf to avoid nested heredoc)
          printf '%s\n' 'import sys' 'import os' '' '# Add the repository root to Python path' "sys.path.insert(0, '${{env.WORKDIR}}')" "sys.path.insert(0, '${{env.WORKDIR}}/frontend')" '' '# Set environment variables' "os.environ['"'PYTHONPATH'"'] = '${{env.WORKDIR}}:${{env.WORKDIR}}/frontend:' + os.environ.get('"'PYTHONPATH'"', '')" > /tmp/drape.py

          # Append the converted notebook content
          cat "/tmp/drape_base.py" >> /tmp/drape.py

          # Create output directory for this example
          mkdir -p /tmp/ci/drape

          # Run the example
          echo "drape" > frontend/.CI
          python3 /tmp/drape.py 2>&1 | tee /tmp/ci/drape/drape.log
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Run fishingknot
        run: |
          echo "Running fishingknot..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -eo pipefail
          cd ${{env.WORKDIR}}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/fishingknot.ipynb" --output "/tmp/fishingknot_base.py"

          # Create the runnable script with proper imports (using printf to avoid nested heredoc)
          printf '%s\n' 'import sys' 'import os' '' '# Add the repository root to Python path' "sys.path.insert(0, '${{env.WORKDIR}}')" "sys.path.insert(0, '${{env.WORKDIR}}/frontend')" '' '# Set environment variables' "os.environ['"'PYTHONPATH'"'] = '${{env.WORKDIR}}:${{env.WORKDIR}}/frontend:' + os.environ.get('"'PYTHONPATH'"', '')" > /tmp/fishingknot.py

          # Append the converted notebook content
          cat "/tmp/fishingknot_base.py" >> /tmp/fishingknot.py

          # Create output directory for this example
          mkdir -p /tmp/ci/fishingknot

          # Run the example
          echo "fishingknot" > frontend/.CI
          python3 /tmp/fishingknot.py 2>&1 | tee /tmp/ci/fishingknot/fishingknot.log
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Run fitting
        run: |
          echo "Running fitting..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -eo pipefail
          cd ${{env.WORKDIR}}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/fitting.ipynb" --output "/tmp/fitting_base.py"

          # Create the runnable script with proper imports (using printf to avoid nested heredoc)
          printf '%s\n' 'import sys' 'import os' '' '# Add the repository root to Python path' "sys.path.insert(0, '${{env.WORKDIR}}')" "sys.path.insert(0, '${{env.WORKDIR}}/frontend')" '' '# Set environment variables' "os.environ['"'PYTHONPATH'"'] = '${{env.WORKDIR}}:${{env.WORKDIR}}/frontend:' + os.environ.get('"'PYTHONPATH'"', '')" > /tmp/fitting.py

          # Append the converted notebook content
          cat "/tmp/fitting_base.py" >> /tmp/fitting.py

          # Create output directory for this example
          mkdir -p /tmp/ci/fitting

          # Run the example
          echo "fitting" > frontend/.CI
          python3 /tmp/fitting.py 2>&1 | tee /tmp/ci/fitting/fitting.log
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Run friction
        run: |
          echo "Running friction..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -eo pipefail
          cd ${{env.WORKDIR}}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/friction.ipynb" --output "/tmp/friction_base.py"

          # Create the runnable script with proper imports (using printf to avoid nested heredoc)
          printf '%s\n' 'import sys' 'import os' '' '# Add the repository root to Python path' "sys.path.insert(0, '${{env.WORKDIR}}')" "sys.path.insert(0, '${{env.WORKDIR}}/frontend')" '' '# Set environment variables' "os.environ['"'PYTHONPATH'"'] = '${{env.WORKDIR}}:${{env.WORKDIR}}/frontend:' + os.environ.get('"'PYTHONPATH'"', '')" > /tmp/friction.py

          # Append the converted notebook content
          cat "/tmp/friction_base.py" >> /tmp/friction.py

          # Create output directory for this example
          mkdir -p /tmp/ci/friction

          # Run the example
          echo "friction" > frontend/.CI
          python3 /tmp/friction.py 2>&1 | tee /tmp/ci/friction/friction.log
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true


      - name: Collect results
        if: success() || failure()
        run: |
          echo "Collecting results from all runs..."
          mkdir -p ci

          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          # Open tunnel for this step
          aws ec2-instance-connect open-tunnel \
            --instance-id "$INSTANCE_ID" \
            --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          # Delete large binary files on remote before copying to save bandwidth
          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            ubuntu@localhost \
            "find ~/.cache/ppf-cts/ci -type f \( -name '*.bin' -o -name '*.pickle' -o -name '*.ply' -o -name '*.gz' \) -delete 2>/dev/null" || true

          # Copy CI output from ppf-cts cache directory
          rsync -avz -e "ssh -i /tmp/ec2key -p 2222 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
            ubuntu@localhost:~/.cache/ppf-cts/ci/ ./ci/ || echo "No ppf-cts CI files found"

          # Also copy logs from /tmp/ci
          rsync -avz -e "ssh -i /tmp/ec2key -p 2222 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
            ubuntu@localhost:/tmp/ci/ ./ci/ || echo "No log files found"

          echo "## Collected Files:"
          ls -laR ci/ | head -100

          # Close tunnel
          kill $TUNNEL_PID 2>/dev/null || true

      - name: Upload artifact
        if: success() || failure()
        uses: actions/upload-artifact@v4
        with:
          name: ci-batch-2
          path: ci
          retention-days: 3

      - name: GPU information
        if: success() || failure()
        run: |
          echo "Getting GPU information..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          # Open tunnel for this step
          aws ec2-instance-connect open-tunnel \
            --instance-id "$INSTANCE_ID" \
            --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            ubuntu@localhost "nvidia-smi" || echo "Failed to get GPU info"

          # Close tunnel
          kill $TUNNEL_PID 2>/dev/null || true

      - name: Re-authenticate for cleanup
        if: always()
        continue-on-error: true
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-duration-seconds: 21600

      - name: Cleanup - Terminate Instance
        if: always()
        continue-on-error: true
        run: |
          if [ -n "${{ steps.instance.outputs.INSTANCE_ID }}" ]; then
            echo "Initiating instance termination: ${{ steps.instance.outputs.INSTANCE_ID }}"
            aws ec2 terminate-instances \
              --instance-ids "${{ steps.instance.outputs.INSTANCE_ID }}" \
              --region "$AWS_REGION" || true
            echo "Termination initiated. Instance will terminate in the background."
          else
            echo "No instance to terminate"
          fi

      - name: Summary
        if: always()
        run: |
          echo "## Workflow Summary - Batch 2"
          echo "- Region: $AWS_REGION"
          echo "- Instance Type: $INSTANCE_TYPE"
          echo "- Branch: $BRANCH"
          echo "- Examples: $EXAMPLES"
          echo "- Instance ID: ${{ steps.instance.outputs.INSTANCE_ID || 'Not launched' }}"

  run-batch-3:
    name: Run Batch 3
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read

    env:
      AWS_REGION: ${{ github.event.inputs.region }}
      INSTANCE_TYPE: ${{ github.event.inputs.instance_type }}
      BRANCH: ${{ github.ref_name }}
      EXAMPLES: "hang needle noodle ribbon"
      WORKDIR: /home/ubuntu
      USER: ubuntu

    steps:
      - name: Show input parameters
        run: |
          echo "## Input Parameters - Batch 3"
          echo "Branch: ${{ github.ref_name }}"
          echo "Instance Type: ${{ github.event.inputs.instance_type }}"
          echo "Region: ${{ github.event.inputs.region }}"
          echo "Examples: hang needle noodle ribbon"

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-duration-seconds: 21600

      - name: Find Deep Learning AMI and network resources
        id: setup
        run: |
          echo "Finding latest Deep Learning AMI with GPU support..."
          AMI_ID=$(aws ec2 describe-images \
            --owners amazon \
            --filters \
              "Name=name,Values=Deep Learning Base OSS Nvidia Driver GPU AMI (Ubuntu 24.04)*" \
              "Name=state,Values=available" \
              "Name=architecture,Values=x86_64" \
            --query 'sort_by(Images, &CreationDate)[-1].ImageId' \
            --region "$AWS_REGION" \
            --output text)

          if [ "$AMI_ID" = "None" ] || [ -z "$AMI_ID" ]; then
            echo "ERROR: Deep Learning AMI not found in region $AWS_REGION"
            exit 1
          fi
          echo "AMI_ID=$AMI_ID" >> $GITHUB_OUTPUT
          echo "Found AMI: $AMI_ID"

          # Get GitHub Actions dedicated VPC, subnet, and security group
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=github-actions-vpc" --query 'Vpcs[0].VpcId' --output text)
          SUBNET_ID=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[0].SubnetId' --output text)
          SG_ID=$(aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$VPC_ID" "Name=group-name,Values=github-actions-sg" --query 'SecurityGroups[0].GroupId' --output text)

          if [ "$VPC_ID" = "None" ] || [ -z "$VPC_ID" ]; then
            echo "ERROR: github-actions-vpc not found in region $AWS_REGION"
            exit 1
          fi

          echo "::add-mask::$VPC_ID"
          echo "::add-mask::$SUBNET_ID"
          echo "::add-mask::$SG_ID"
          echo "SUBNET_ID=$SUBNET_ID" >> $GITHUB_OUTPUT
          echo "SG_ID=$SG_ID" >> $GITHUB_OUTPUT
          echo "VPC: $VPC_ID, Subnet: $SUBNET_ID, SG: $SG_ID"

      - name: Generate unique identifiers and SSH key
        id: ids
        run: |
          TIMESTAMP=$(date +%Y%m%d%H%M%S)
          RANDOM_SUFFIX=$(head /dev/urandom | tr -dc a-z0-9 | head -c 6)
          TEMP_INSTANCE_ID="temp-${TIMESTAMP}-${RANDOM_SUFFIX}"
          echo "TIMESTAMP=$TIMESTAMP" >> $GITHUB_OUTPUT
          echo "TEMP_INSTANCE_ID=$TEMP_INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "Temporary Instance ID: $TEMP_INSTANCE_ID"

          # Generate SSH key early so we can embed it in user-data
          rm -f /tmp/ec2key /tmp/ec2key.pub
          ssh-keygen -t rsa -f /tmp/ec2key -N "" -q
          echo "SSH key generated"

      - name: Create user data script
        run: |
          SSH_PUBKEY=$(cat /tmp/ec2key.pub)
          cat > /tmp/user-data.sh << EOF
          #!/bin/bash
          set -x
          exec > >(tee /var/log/user-data.log) 2>&1

          echo "=== User Data Script Started ==="

          # Wait for system to be ready
          sleep 5

          # Setup SSH key for persistent authentication (no 60s expiry)
          mkdir -p /home/ubuntu/.ssh
          echo "${SSH_PUBKEY}" >> /home/ubuntu/.ssh/authorized_keys
          chown -R ubuntu:ubuntu /home/ubuntu/.ssh
          chmod 700 /home/ubuntu/.ssh
          chmod 600 /home/ubuntu/.ssh/authorized_keys
          echo "SSH key installed permanently"

          # Verify nvidia-smi is available
          if command -v nvidia-smi &> /dev/null; then
              echo "NVIDIA drivers confirmed"
              nvidia-smi
          else
              echo "Warning: nvidia-smi not found"
          fi

          # Create workspace directory
          mkdir -p /home/ubuntu/workspace
          chown -R ubuntu:ubuntu /home/ubuntu/workspace

          nvidia-smi | tee /tmp/nvidia-smi-output.txt
          touch /tmp/setup-complete
          echo "=== User Data Script Complete ==="
          EOF

      - name: Launch EC2 instance
        id: instance
        run: |
          echo "Launching EC2 instance..."

          # Base64 encode for AWS
          USER_DATA=$(base64 -w 0 /tmp/user-data.sh)

          INSTANCE_ID=$(aws ec2 run-instances \
            --image-id "${{ steps.setup.outputs.AMI_ID }}" \
            --instance-type "$INSTANCE_TYPE" \
            --subnet-id "${{ steps.setup.outputs.SUBNET_ID }}" \
            --security-group-ids "${{ steps.setup.outputs.SG_ID }}" \
            --associate-public-ip-address \
            --user-data "$USER_DATA" \
            --block-device-mappings "DeviceName=/dev/sda1,Ebs={VolumeSize=256,VolumeType=gp3,DeleteOnTermination=true}" \
            --tag-specifications \
              "ResourceType=instance,Tags=[\
                {Key=Name,Value=gpu-runner-batch-3-${{ steps.ids.outputs.TIMESTAMP }}},\
                {Key=ManagedBy,Value=GitHubActions},\
                {Key=Purpose,Value=GPURunner},\
                {Key=Workflow,Value=${{ github.workflow }}},\
                {Key=RunId,Value=${{ github.run_id }}},\
                {Key=Branch,Value=${{ env.BRANCH }}},\
                {Key=Batch,Value=3}\
              ]" \
              "ResourceType=volume,Tags=[\
                {Key=Name,Value=gpu-runner-batch-3-${{ steps.ids.outputs.TIMESTAMP }}-volume},\
                {Key=ManagedBy,Value=GitHubActions},\
                {Key=Purpose,Value=GPURunner},\
                {Key=Batch,Value=3}\
              ]" \
            --instance-initiated-shutdown-behavior terminate \
            --query 'Instances[0].InstanceId' \
            --region "$AWS_REGION" \
            --output text)

          echo "INSTANCE_ID=$INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "$INSTANCE_ID" > /tmp/instance_id.txt
          echo "Instance launched: $INSTANCE_ID"

      - name: Wait for instance and user-data
        run: |
          INSTANCE_ID=$(cat /tmp/instance_id.txt)
          echo "Waiting for instance to be running..."
          aws ec2 wait instance-running --instance-ids "$INSTANCE_ID" --region "$AWS_REGION"

          echo "Instance is running, waiting for user-data to complete..."

          # Wait for setup-complete (user-data finished, SSH key installed)
          MAX_ATTEMPTS=60
          for i in $(seq 1 $MAX_ATTEMPTS); do
            # Open tunnel temporarily
            aws ec2-instance-connect open-tunnel \
              --instance-id "$INSTANCE_ID" \
              --local-port 2222 &
            TUNNEL_PID=$!
            sleep 3

            # Check if setup is complete (includes SSH connectivity check)
            if ssh -i /tmp/ec2key -p 2222 \
              -o StrictHostKeyChecking=no \
              -o UserKnownHostsFile=/dev/null \
              -o ConnectTimeout=5 \
              ubuntu@localhost "test -f /tmp/setup-complete && echo READY" 2>/dev/null | grep -q READY; then
              echo "Instance setup completed on attempt $i"
              kill $TUNNEL_PID 2>/dev/null || true
              break
            fi

            kill $TUNNEL_PID 2>/dev/null || true

            if [ $i -eq $MAX_ATTEMPTS ]; then
              echo "Setup timeout after $MAX_ATTEMPTS attempts, continuing anyway..."
              break
            fi

            echo "Attempt $i/$MAX_ATTEMPTS: Setup not complete, waiting 10s..."
            sleep 10
          done

      - name: Create archive of repository
        run: |
          echo "Creating repository archive..."
          git archive --format=tar.gz --output=/tmp/repo.tar.gz HEAD

      - name: Transfer repository to instance
        run: |
          echo "Transferring repository to instance..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          rsync -avz -e "ssh -i /tmp/ec2key -p 2222 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
            /tmp/repo.tar.gz ubuntu@localhost:${{env.WORKDIR}}/

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            ubuntu@localhost \
            "cd ${{env.WORKDIR}} && tar -xzf repo.tar.gz && rm repo.tar.gz"

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Setup Python environment and run warmup
        run: |
          echo "Setting up Python environment and running warmup.py..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -e
          cd ${{env.WORKDIR}}

          # Run warmup.py
          echo "Running warmup.py..."
          python3 warmup.py --skip-confirmation

          echo "Warmup completed"
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Build Rust project
        run: |
          echo "Building Rust project with cargo..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -e
          cd ${{env.WORKDIR}}

          # Setup Rust environment
          source "$HOME/.cargo/env"

          # Build the project
          echo "Running cargo build --release..."
          cargo build --release

          echo "Cargo build completed"
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Setup CI directory
        run: |
          INSTANCE_ID=$(cat /tmp/instance_id.txt)
          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5
          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            ubuntu@localhost "mkdir -p /tmp/ci"
          kill $TUNNEL_PID 2>/dev/null || true

      - name: Run hang
        run: |
          echo "Running hang..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -eo pipefail
          cd ${{env.WORKDIR}}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/hang.ipynb" --output "/tmp/hang_base.py"

          # Create the runnable script with proper imports (using printf to avoid nested heredoc)
          printf '%s\n' 'import sys' 'import os' '' '# Add the repository root to Python path' "sys.path.insert(0, '${{env.WORKDIR}}')" "sys.path.insert(0, '${{env.WORKDIR}}/frontend')" '' '# Set environment variables' "os.environ['"'PYTHONPATH'"'] = '${{env.WORKDIR}}:${{env.WORKDIR}}/frontend:' + os.environ.get('"'PYTHONPATH'"', '')" > /tmp/hang.py

          # Append the converted notebook content
          cat "/tmp/hang_base.py" >> /tmp/hang.py

          # Create output directory for this example
          mkdir -p /tmp/ci/hang

          # Run the example
          echo "hang" > frontend/.CI
          python3 /tmp/hang.py 2>&1 | tee /tmp/ci/hang/hang.log
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Run needle
        run: |
          echo "Running needle..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -eo pipefail
          cd ${{env.WORKDIR}}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/needle.ipynb" --output "/tmp/needle_base.py"

          # Create the runnable script with proper imports (using printf to avoid nested heredoc)
          printf '%s\n' 'import sys' 'import os' '' '# Add the repository root to Python path' "sys.path.insert(0, '${{env.WORKDIR}}')" "sys.path.insert(0, '${{env.WORKDIR}}/frontend')" '' '# Set environment variables' "os.environ['"'PYTHONPATH'"'] = '${{env.WORKDIR}}:${{env.WORKDIR}}/frontend:' + os.environ.get('"'PYTHONPATH'"', '')" > /tmp/needle.py

          # Append the converted notebook content
          cat "/tmp/needle_base.py" >> /tmp/needle.py

          # Create output directory for this example
          mkdir -p /tmp/ci/needle

          # Run the example
          echo "needle" > frontend/.CI
          python3 /tmp/needle.py 2>&1 | tee /tmp/ci/needle/needle.log
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Run noodle
        run: |
          echo "Running noodle..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -eo pipefail
          cd ${{env.WORKDIR}}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/noodle.ipynb" --output "/tmp/noodle_base.py"

          # Create the runnable script with proper imports (using printf to avoid nested heredoc)
          printf '%s\n' 'import sys' 'import os' '' '# Add the repository root to Python path' "sys.path.insert(0, '${{env.WORKDIR}}')" "sys.path.insert(0, '${{env.WORKDIR}}/frontend')" '' '# Set environment variables' "os.environ['"'PYTHONPATH'"'] = '${{env.WORKDIR}}:${{env.WORKDIR}}/frontend:' + os.environ.get('"'PYTHONPATH'"', '')" > /tmp/noodle.py

          # Append the converted notebook content
          cat "/tmp/noodle_base.py" >> /tmp/noodle.py

          # Create output directory for this example
          mkdir -p /tmp/ci/noodle

          # Run the example
          echo "noodle" > frontend/.CI
          python3 /tmp/noodle.py 2>&1 | tee /tmp/ci/noodle/noodle.log
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Run ribbon
        run: |
          echo "Running ribbon..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -eo pipefail
          cd ${{env.WORKDIR}}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/ribbon.ipynb" --output "/tmp/ribbon_base.py"

          # Create the runnable script with proper imports (using printf to avoid nested heredoc)
          printf '%s\n' 'import sys' 'import os' '' '# Add the repository root to Python path' "sys.path.insert(0, '${{env.WORKDIR}}')" "sys.path.insert(0, '${{env.WORKDIR}}/frontend')" '' '# Set environment variables' "os.environ['"'PYTHONPATH'"'] = '${{env.WORKDIR}}:${{env.WORKDIR}}/frontend:' + os.environ.get('"'PYTHONPATH'"', '')" > /tmp/ribbon.py

          # Append the converted notebook content
          cat "/tmp/ribbon_base.py" >> /tmp/ribbon.py

          # Create output directory for this example
          mkdir -p /tmp/ci/ribbon

          # Run the example
          echo "ribbon" > frontend/.CI
          python3 /tmp/ribbon.py 2>&1 | tee /tmp/ci/ribbon/ribbon.log
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true


      - name: Collect results
        if: success() || failure()
        run: |
          echo "Collecting results from all runs..."
          mkdir -p ci

          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          # Open tunnel for this step
          aws ec2-instance-connect open-tunnel \
            --instance-id "$INSTANCE_ID" \
            --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          # Delete large binary files on remote before copying to save bandwidth
          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            ubuntu@localhost \
            "find ~/.cache/ppf-cts/ci -type f \( -name '*.bin' -o -name '*.pickle' -o -name '*.ply' -o -name '*.gz' \) -delete 2>/dev/null" || true

          # Copy CI output from ppf-cts cache directory
          rsync -avz -e "ssh -i /tmp/ec2key -p 2222 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
            ubuntu@localhost:~/.cache/ppf-cts/ci/ ./ci/ || echo "No ppf-cts CI files found"

          # Also copy logs from /tmp/ci
          rsync -avz -e "ssh -i /tmp/ec2key -p 2222 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
            ubuntu@localhost:/tmp/ci/ ./ci/ || echo "No log files found"

          echo "## Collected Files:"
          ls -laR ci/ | head -100

          # Close tunnel
          kill $TUNNEL_PID 2>/dev/null || true

      - name: Upload artifact
        if: success() || failure()
        uses: actions/upload-artifact@v4
        with:
          name: ci-batch-3
          path: ci
          retention-days: 3

      - name: GPU information
        if: success() || failure()
        run: |
          echo "Getting GPU information..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          # Open tunnel for this step
          aws ec2-instance-connect open-tunnel \
            --instance-id "$INSTANCE_ID" \
            --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            ubuntu@localhost "nvidia-smi" || echo "Failed to get GPU info"

          # Close tunnel
          kill $TUNNEL_PID 2>/dev/null || true

      - name: Re-authenticate for cleanup
        if: always()
        continue-on-error: true
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-duration-seconds: 21600

      - name: Cleanup - Terminate Instance
        if: always()
        continue-on-error: true
        run: |
          if [ -n "${{ steps.instance.outputs.INSTANCE_ID }}" ]; then
            echo "Initiating instance termination: ${{ steps.instance.outputs.INSTANCE_ID }}"
            aws ec2 terminate-instances \
              --instance-ids "${{ steps.instance.outputs.INSTANCE_ID }}" \
              --region "$AWS_REGION" || true
            echo "Termination initiated. Instance will terminate in the background."
          else
            echo "No instance to terminate"
          fi

      - name: Summary
        if: always()
        run: |
          echo "## Workflow Summary - Batch 3"
          echo "- Region: $AWS_REGION"
          echo "- Instance Type: $INSTANCE_TYPE"
          echo "- Branch: $BRANCH"
          echo "- Examples: $EXAMPLES"
          echo "- Instance ID: ${{ steps.instance.outputs.INSTANCE_ID || 'Not launched' }}"

  run-batch-4:
    name: Run Batch 4
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read

    env:
      AWS_REGION: ${{ github.event.inputs.region }}
      INSTANCE_TYPE: ${{ github.event.inputs.instance_type }}
      BRANCH: ${{ github.ref_name }}
      EXAMPLES: "roller stack trampoline trapped"
      WORKDIR: /home/ubuntu
      USER: ubuntu

    steps:
      - name: Show input parameters
        run: |
          echo "## Input Parameters - Batch 4"
          echo "Branch: ${{ github.ref_name }}"
          echo "Instance Type: ${{ github.event.inputs.instance_type }}"
          echo "Region: ${{ github.event.inputs.region }}"
          echo "Examples: roller stack trampoline trapped"

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-duration-seconds: 21600

      - name: Find Deep Learning AMI and network resources
        id: setup
        run: |
          echo "Finding latest Deep Learning AMI with GPU support..."
          AMI_ID=$(aws ec2 describe-images \
            --owners amazon \
            --filters \
              "Name=name,Values=Deep Learning Base OSS Nvidia Driver GPU AMI (Ubuntu 24.04)*" \
              "Name=state,Values=available" \
              "Name=architecture,Values=x86_64" \
            --query 'sort_by(Images, &CreationDate)[-1].ImageId' \
            --region "$AWS_REGION" \
            --output text)

          if [ "$AMI_ID" = "None" ] || [ -z "$AMI_ID" ]; then
            echo "ERROR: Deep Learning AMI not found in region $AWS_REGION"
            exit 1
          fi
          echo "AMI_ID=$AMI_ID" >> $GITHUB_OUTPUT
          echo "Found AMI: $AMI_ID"

          # Get GitHub Actions dedicated VPC, subnet, and security group
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=github-actions-vpc" --query 'Vpcs[0].VpcId' --output text)
          SUBNET_ID=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[0].SubnetId' --output text)
          SG_ID=$(aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$VPC_ID" "Name=group-name,Values=github-actions-sg" --query 'SecurityGroups[0].GroupId' --output text)

          if [ "$VPC_ID" = "None" ] || [ -z "$VPC_ID" ]; then
            echo "ERROR: github-actions-vpc not found in region $AWS_REGION"
            exit 1
          fi

          echo "::add-mask::$VPC_ID"
          echo "::add-mask::$SUBNET_ID"
          echo "::add-mask::$SG_ID"
          echo "SUBNET_ID=$SUBNET_ID" >> $GITHUB_OUTPUT
          echo "SG_ID=$SG_ID" >> $GITHUB_OUTPUT
          echo "VPC: $VPC_ID, Subnet: $SUBNET_ID, SG: $SG_ID"

      - name: Generate unique identifiers and SSH key
        id: ids
        run: |
          TIMESTAMP=$(date +%Y%m%d%H%M%S)
          RANDOM_SUFFIX=$(head /dev/urandom | tr -dc a-z0-9 | head -c 6)
          TEMP_INSTANCE_ID="temp-${TIMESTAMP}-${RANDOM_SUFFIX}"
          echo "TIMESTAMP=$TIMESTAMP" >> $GITHUB_OUTPUT
          echo "TEMP_INSTANCE_ID=$TEMP_INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "Temporary Instance ID: $TEMP_INSTANCE_ID"

          # Generate SSH key early so we can embed it in user-data
          rm -f /tmp/ec2key /tmp/ec2key.pub
          ssh-keygen -t rsa -f /tmp/ec2key -N "" -q
          echo "SSH key generated"

      - name: Create user data script
        run: |
          SSH_PUBKEY=$(cat /tmp/ec2key.pub)
          cat > /tmp/user-data.sh << EOF
          #!/bin/bash
          set -x
          exec > >(tee /var/log/user-data.log) 2>&1

          echo "=== User Data Script Started ==="

          # Wait for system to be ready
          sleep 5

          # Setup SSH key for persistent authentication (no 60s expiry)
          mkdir -p /home/ubuntu/.ssh
          echo "${SSH_PUBKEY}" >> /home/ubuntu/.ssh/authorized_keys
          chown -R ubuntu:ubuntu /home/ubuntu/.ssh
          chmod 700 /home/ubuntu/.ssh
          chmod 600 /home/ubuntu/.ssh/authorized_keys
          echo "SSH key installed permanently"

          # Verify nvidia-smi is available
          if command -v nvidia-smi &> /dev/null; then
              echo "NVIDIA drivers confirmed"
              nvidia-smi
          else
              echo "Warning: nvidia-smi not found"
          fi

          # Create workspace directory
          mkdir -p /home/ubuntu/workspace
          chown -R ubuntu:ubuntu /home/ubuntu/workspace

          nvidia-smi | tee /tmp/nvidia-smi-output.txt
          touch /tmp/setup-complete
          echo "=== User Data Script Complete ==="
          EOF

      - name: Launch EC2 instance
        id: instance
        run: |
          echo "Launching EC2 instance..."

          # Base64 encode for AWS
          USER_DATA=$(base64 -w 0 /tmp/user-data.sh)

          INSTANCE_ID=$(aws ec2 run-instances \
            --image-id "${{ steps.setup.outputs.AMI_ID }}" \
            --instance-type "$INSTANCE_TYPE" \
            --subnet-id "${{ steps.setup.outputs.SUBNET_ID }}" \
            --security-group-ids "${{ steps.setup.outputs.SG_ID }}" \
            --associate-public-ip-address \
            --user-data "$USER_DATA" \
            --block-device-mappings "DeviceName=/dev/sda1,Ebs={VolumeSize=256,VolumeType=gp3,DeleteOnTermination=true}" \
            --tag-specifications \
              "ResourceType=instance,Tags=[\
                {Key=Name,Value=gpu-runner-batch-4-${{ steps.ids.outputs.TIMESTAMP }}},\
                {Key=ManagedBy,Value=GitHubActions},\
                {Key=Purpose,Value=GPURunner},\
                {Key=Workflow,Value=${{ github.workflow }}},\
                {Key=RunId,Value=${{ github.run_id }}},\
                {Key=Branch,Value=${{ env.BRANCH }}},\
                {Key=Batch,Value=4}\
              ]" \
              "ResourceType=volume,Tags=[\
                {Key=Name,Value=gpu-runner-batch-4-${{ steps.ids.outputs.TIMESTAMP }}-volume},\
                {Key=ManagedBy,Value=GitHubActions},\
                {Key=Purpose,Value=GPURunner},\
                {Key=Batch,Value=4}\
              ]" \
            --instance-initiated-shutdown-behavior terminate \
            --query 'Instances[0].InstanceId' \
            --region "$AWS_REGION" \
            --output text)

          echo "INSTANCE_ID=$INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "$INSTANCE_ID" > /tmp/instance_id.txt
          echo "Instance launched: $INSTANCE_ID"

      - name: Wait for instance and user-data
        run: |
          INSTANCE_ID=$(cat /tmp/instance_id.txt)
          echo "Waiting for instance to be running..."
          aws ec2 wait instance-running --instance-ids "$INSTANCE_ID" --region "$AWS_REGION"

          echo "Instance is running, waiting for user-data to complete..."

          # Wait for setup-complete (user-data finished, SSH key installed)
          MAX_ATTEMPTS=60
          for i in $(seq 1 $MAX_ATTEMPTS); do
            # Open tunnel temporarily
            aws ec2-instance-connect open-tunnel \
              --instance-id "$INSTANCE_ID" \
              --local-port 2222 &
            TUNNEL_PID=$!
            sleep 3

            # Check if setup is complete (includes SSH connectivity check)
            if ssh -i /tmp/ec2key -p 2222 \
              -o StrictHostKeyChecking=no \
              -o UserKnownHostsFile=/dev/null \
              -o ConnectTimeout=5 \
              ubuntu@localhost "test -f /tmp/setup-complete && echo READY" 2>/dev/null | grep -q READY; then
              echo "Instance setup completed on attempt $i"
              kill $TUNNEL_PID 2>/dev/null || true
              break
            fi

            kill $TUNNEL_PID 2>/dev/null || true

            if [ $i -eq $MAX_ATTEMPTS ]; then
              echo "Setup timeout after $MAX_ATTEMPTS attempts, continuing anyway..."
              break
            fi

            echo "Attempt $i/$MAX_ATTEMPTS: Setup not complete, waiting 10s..."
            sleep 10
          done

      - name: Create archive of repository
        run: |
          echo "Creating repository archive..."
          git archive --format=tar.gz --output=/tmp/repo.tar.gz HEAD

      - name: Transfer repository to instance
        run: |
          echo "Transferring repository to instance..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          rsync -avz -e "ssh -i /tmp/ec2key -p 2222 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
            /tmp/repo.tar.gz ubuntu@localhost:${{env.WORKDIR}}/

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            ubuntu@localhost \
            "cd ${{env.WORKDIR}} && tar -xzf repo.tar.gz && rm repo.tar.gz"

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Setup Python environment and run warmup
        run: |
          echo "Setting up Python environment and running warmup.py..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -e
          cd ${{env.WORKDIR}}

          # Run warmup.py
          echo "Running warmup.py..."
          python3 warmup.py --skip-confirmation

          echo "Warmup completed"
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Build Rust project
        run: |
          echo "Building Rust project with cargo..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -e
          cd ${{env.WORKDIR}}

          # Setup Rust environment
          source "$HOME/.cargo/env"

          # Build the project
          echo "Running cargo build --release..."
          cargo build --release

          echo "Cargo build completed"
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Setup CI directory
        run: |
          INSTANCE_ID=$(cat /tmp/instance_id.txt)
          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5
          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            ubuntu@localhost "mkdir -p /tmp/ci"
          kill $TUNNEL_PID 2>/dev/null || true

      - name: Run roller
        run: |
          echo "Running roller..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -eo pipefail
          cd ${{env.WORKDIR}}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/roller.ipynb" --output "/tmp/roller_base.py"

          # Create the runnable script with proper imports (using printf to avoid nested heredoc)
          printf '%s\n' 'import sys' 'import os' '' '# Add the repository root to Python path' "sys.path.insert(0, '${{env.WORKDIR}}')" "sys.path.insert(0, '${{env.WORKDIR}}/frontend')" '' '# Set environment variables' "os.environ['"'PYTHONPATH'"'] = '${{env.WORKDIR}}:${{env.WORKDIR}}/frontend:' + os.environ.get('"'PYTHONPATH'"', '')" > /tmp/roller.py

          # Append the converted notebook content
          cat "/tmp/roller_base.py" >> /tmp/roller.py

          # Create output directory for this example
          mkdir -p /tmp/ci/roller

          # Run the example
          echo "roller" > frontend/.CI
          python3 /tmp/roller.py 2>&1 | tee /tmp/ci/roller/roller.log
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Run stack
        run: |
          echo "Running stack..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -eo pipefail
          cd ${{env.WORKDIR}}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/stack.ipynb" --output "/tmp/stack_base.py"

          # Create the runnable script with proper imports (using printf to avoid nested heredoc)
          printf '%s\n' 'import sys' 'import os' '' '# Add the repository root to Python path' "sys.path.insert(0, '${{env.WORKDIR}}')" "sys.path.insert(0, '${{env.WORKDIR}}/frontend')" '' '# Set environment variables' "os.environ['"'PYTHONPATH'"'] = '${{env.WORKDIR}}:${{env.WORKDIR}}/frontend:' + os.environ.get('"'PYTHONPATH'"', '')" > /tmp/stack.py

          # Append the converted notebook content
          cat "/tmp/stack_base.py" >> /tmp/stack.py

          # Create output directory for this example
          mkdir -p /tmp/ci/stack

          # Run the example
          echo "stack" > frontend/.CI
          python3 /tmp/stack.py 2>&1 | tee /tmp/ci/stack/stack.log
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Run trampoline
        run: |
          echo "Running trampoline..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -eo pipefail
          cd ${{env.WORKDIR}}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/trampoline.ipynb" --output "/tmp/trampoline_base.py"

          # Create the runnable script with proper imports (using printf to avoid nested heredoc)
          printf '%s\n' 'import sys' 'import os' '' '# Add the repository root to Python path' "sys.path.insert(0, '${{env.WORKDIR}}')" "sys.path.insert(0, '${{env.WORKDIR}}/frontend')" '' '# Set environment variables' "os.environ['"'PYTHONPATH'"'] = '${{env.WORKDIR}}:${{env.WORKDIR}}/frontend:' + os.environ.get('"'PYTHONPATH'"', '')" > /tmp/trampoline.py

          # Append the converted notebook content
          cat "/tmp/trampoline_base.py" >> /tmp/trampoline.py

          # Create output directory for this example
          mkdir -p /tmp/ci/trampoline

          # Run the example
          echo "trampoline" > frontend/.CI
          python3 /tmp/trampoline.py 2>&1 | tee /tmp/ci/trampoline/trampoline.log
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Run trapped
        run: |
          echo "Running trapped..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -eo pipefail
          cd ${{env.WORKDIR}}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/trapped.ipynb" --output "/tmp/trapped_base.py"

          # Create the runnable script with proper imports (using printf to avoid nested heredoc)
          printf '%s\n' 'import sys' 'import os' '' '# Add the repository root to Python path' "sys.path.insert(0, '${{env.WORKDIR}}')" "sys.path.insert(0, '${{env.WORKDIR}}/frontend')" '' '# Set environment variables' "os.environ['"'PYTHONPATH'"'] = '${{env.WORKDIR}}:${{env.WORKDIR}}/frontend:' + os.environ.get('"'PYTHONPATH'"', '')" > /tmp/trapped.py

          # Append the converted notebook content
          cat "/tmp/trapped_base.py" >> /tmp/trapped.py

          # Create output directory for this example
          mkdir -p /tmp/ci/trapped

          # Run the example
          echo "trapped" > frontend/.CI
          python3 /tmp/trapped.py 2>&1 | tee /tmp/ci/trapped/trapped.log
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true


      - name: Collect results
        if: success() || failure()
        run: |
          echo "Collecting results from all runs..."
          mkdir -p ci

          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          # Open tunnel for this step
          aws ec2-instance-connect open-tunnel \
            --instance-id "$INSTANCE_ID" \
            --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          # Delete large binary files on remote before copying to save bandwidth
          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            ubuntu@localhost \
            "find ~/.cache/ppf-cts/ci -type f \( -name '*.bin' -o -name '*.pickle' -o -name '*.ply' -o -name '*.gz' \) -delete 2>/dev/null" || true

          # Copy CI output from ppf-cts cache directory
          rsync -avz -e "ssh -i /tmp/ec2key -p 2222 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
            ubuntu@localhost:~/.cache/ppf-cts/ci/ ./ci/ || echo "No ppf-cts CI files found"

          # Also copy logs from /tmp/ci
          rsync -avz -e "ssh -i /tmp/ec2key -p 2222 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
            ubuntu@localhost:/tmp/ci/ ./ci/ || echo "No log files found"

          echo "## Collected Files:"
          ls -laR ci/ | head -100

          # Close tunnel
          kill $TUNNEL_PID 2>/dev/null || true

      - name: Upload artifact
        if: success() || failure()
        uses: actions/upload-artifact@v4
        with:
          name: ci-batch-4
          path: ci
          retention-days: 3

      - name: GPU information
        if: success() || failure()
        run: |
          echo "Getting GPU information..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          # Open tunnel for this step
          aws ec2-instance-connect open-tunnel \
            --instance-id "$INSTANCE_ID" \
            --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            ubuntu@localhost "nvidia-smi" || echo "Failed to get GPU info"

          # Close tunnel
          kill $TUNNEL_PID 2>/dev/null || true

      - name: Re-authenticate for cleanup
        if: always()
        continue-on-error: true
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-duration-seconds: 21600

      - name: Cleanup - Terminate Instance
        if: always()
        continue-on-error: true
        run: |
          if [ -n "${{ steps.instance.outputs.INSTANCE_ID }}" ]; then
            echo "Initiating instance termination: ${{ steps.instance.outputs.INSTANCE_ID }}"
            aws ec2 terminate-instances \
              --instance-ids "${{ steps.instance.outputs.INSTANCE_ID }}" \
              --region "$AWS_REGION" || true
            echo "Termination initiated. Instance will terminate in the background."
          else
            echo "No instance to terminate"
          fi

      - name: Summary
        if: always()
        run: |
          echo "## Workflow Summary - Batch 4"
          echo "- Region: $AWS_REGION"
          echo "- Instance Type: $INSTANCE_TYPE"
          echo "- Branch: $BRANCH"
          echo "- Examples: $EXAMPLES"
          echo "- Instance ID: ${{ steps.instance.outputs.INSTANCE_ID || 'Not launched' }}"

  run-batch-5:
    name: Run Batch 5
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read

    env:
      AWS_REGION: ${{ github.event.inputs.region }}
      INSTANCE_TYPE: ${{ github.event.inputs.instance_type }}
      BRANCH: ${{ github.ref_name }}
      EXAMPLES: "twist five-twist woven yarn"
      WORKDIR: /home/ubuntu
      USER: ubuntu

    steps:
      - name: Show input parameters
        run: |
          echo "## Input Parameters - Batch 5"
          echo "Branch: ${{ github.ref_name }}"
          echo "Instance Type: ${{ github.event.inputs.instance_type }}"
          echo "Region: ${{ github.event.inputs.region }}"
          echo "Examples: twist five-twist woven yarn"

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-duration-seconds: 21600

      - name: Find Deep Learning AMI and network resources
        id: setup
        run: |
          echo "Finding latest Deep Learning AMI with GPU support..."
          AMI_ID=$(aws ec2 describe-images \
            --owners amazon \
            --filters \
              "Name=name,Values=Deep Learning Base OSS Nvidia Driver GPU AMI (Ubuntu 24.04)*" \
              "Name=state,Values=available" \
              "Name=architecture,Values=x86_64" \
            --query 'sort_by(Images, &CreationDate)[-1].ImageId' \
            --region "$AWS_REGION" \
            --output text)

          if [ "$AMI_ID" = "None" ] || [ -z "$AMI_ID" ]; then
            echo "ERROR: Deep Learning AMI not found in region $AWS_REGION"
            exit 1
          fi
          echo "AMI_ID=$AMI_ID" >> $GITHUB_OUTPUT
          echo "Found AMI: $AMI_ID"

          # Get GitHub Actions dedicated VPC, subnet, and security group
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=github-actions-vpc" --query 'Vpcs[0].VpcId' --output text)
          SUBNET_ID=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[0].SubnetId' --output text)
          SG_ID=$(aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$VPC_ID" "Name=group-name,Values=github-actions-sg" --query 'SecurityGroups[0].GroupId' --output text)

          if [ "$VPC_ID" = "None" ] || [ -z "$VPC_ID" ]; then
            echo "ERROR: github-actions-vpc not found in region $AWS_REGION"
            exit 1
          fi

          echo "::add-mask::$VPC_ID"
          echo "::add-mask::$SUBNET_ID"
          echo "::add-mask::$SG_ID"
          echo "SUBNET_ID=$SUBNET_ID" >> $GITHUB_OUTPUT
          echo "SG_ID=$SG_ID" >> $GITHUB_OUTPUT
          echo "VPC: $VPC_ID, Subnet: $SUBNET_ID, SG: $SG_ID"

      - name: Generate unique identifiers and SSH key
        id: ids
        run: |
          TIMESTAMP=$(date +%Y%m%d%H%M%S)
          RANDOM_SUFFIX=$(head /dev/urandom | tr -dc a-z0-9 | head -c 6)
          TEMP_INSTANCE_ID="temp-${TIMESTAMP}-${RANDOM_SUFFIX}"
          echo "TIMESTAMP=$TIMESTAMP" >> $GITHUB_OUTPUT
          echo "TEMP_INSTANCE_ID=$TEMP_INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "Temporary Instance ID: $TEMP_INSTANCE_ID"

          # Generate SSH key early so we can embed it in user-data
          rm -f /tmp/ec2key /tmp/ec2key.pub
          ssh-keygen -t rsa -f /tmp/ec2key -N "" -q
          echo "SSH key generated"

      - name: Create user data script
        run: |
          SSH_PUBKEY=$(cat /tmp/ec2key.pub)
          cat > /tmp/user-data.sh << EOF
          #!/bin/bash
          set -x
          exec > >(tee /var/log/user-data.log) 2>&1

          echo "=== User Data Script Started ==="

          # Wait for system to be ready
          sleep 5

          # Setup SSH key for persistent authentication (no 60s expiry)
          mkdir -p /home/ubuntu/.ssh
          echo "${SSH_PUBKEY}" >> /home/ubuntu/.ssh/authorized_keys
          chown -R ubuntu:ubuntu /home/ubuntu/.ssh
          chmod 700 /home/ubuntu/.ssh
          chmod 600 /home/ubuntu/.ssh/authorized_keys
          echo "SSH key installed permanently"

          # Verify nvidia-smi is available
          if command -v nvidia-smi &> /dev/null; then
              echo "NVIDIA drivers confirmed"
              nvidia-smi
          else
              echo "Warning: nvidia-smi not found"
          fi

          # Create workspace directory
          mkdir -p /home/ubuntu/workspace
          chown -R ubuntu:ubuntu /home/ubuntu/workspace

          nvidia-smi | tee /tmp/nvidia-smi-output.txt
          touch /tmp/setup-complete
          echo "=== User Data Script Complete ==="
          EOF

      - name: Launch EC2 instance
        id: instance
        run: |
          echo "Launching EC2 instance..."

          # Base64 encode for AWS
          USER_DATA=$(base64 -w 0 /tmp/user-data.sh)

          INSTANCE_ID=$(aws ec2 run-instances \
            --image-id "${{ steps.setup.outputs.AMI_ID }}" \
            --instance-type "$INSTANCE_TYPE" \
            --subnet-id "${{ steps.setup.outputs.SUBNET_ID }}" \
            --security-group-ids "${{ steps.setup.outputs.SG_ID }}" \
            --associate-public-ip-address \
            --user-data "$USER_DATA" \
            --block-device-mappings "DeviceName=/dev/sda1,Ebs={VolumeSize=256,VolumeType=gp3,DeleteOnTermination=true}" \
            --tag-specifications \
              "ResourceType=instance,Tags=[\
                {Key=Name,Value=gpu-runner-batch-5-${{ steps.ids.outputs.TIMESTAMP }}},\
                {Key=ManagedBy,Value=GitHubActions},\
                {Key=Purpose,Value=GPURunner},\
                {Key=Workflow,Value=${{ github.workflow }}},\
                {Key=RunId,Value=${{ github.run_id }}},\
                {Key=Branch,Value=${{ env.BRANCH }}},\
                {Key=Batch,Value=5}\
              ]" \
              "ResourceType=volume,Tags=[\
                {Key=Name,Value=gpu-runner-batch-5-${{ steps.ids.outputs.TIMESTAMP }}-volume},\
                {Key=ManagedBy,Value=GitHubActions},\
                {Key=Purpose,Value=GPURunner},\
                {Key=Batch,Value=5}\
              ]" \
            --instance-initiated-shutdown-behavior terminate \
            --query 'Instances[0].InstanceId' \
            --region "$AWS_REGION" \
            --output text)

          echo "INSTANCE_ID=$INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "$INSTANCE_ID" > /tmp/instance_id.txt
          echo "Instance launched: $INSTANCE_ID"

      - name: Wait for instance and user-data
        run: |
          INSTANCE_ID=$(cat /tmp/instance_id.txt)
          echo "Waiting for instance to be running..."
          aws ec2 wait instance-running --instance-ids "$INSTANCE_ID" --region "$AWS_REGION"

          echo "Instance is running, waiting for user-data to complete..."

          # Wait for setup-complete (user-data finished, SSH key installed)
          MAX_ATTEMPTS=60
          for i in $(seq 1 $MAX_ATTEMPTS); do
            # Open tunnel temporarily
            aws ec2-instance-connect open-tunnel \
              --instance-id "$INSTANCE_ID" \
              --local-port 2222 &
            TUNNEL_PID=$!
            sleep 3

            # Check if setup is complete (includes SSH connectivity check)
            if ssh -i /tmp/ec2key -p 2222 \
              -o StrictHostKeyChecking=no \
              -o UserKnownHostsFile=/dev/null \
              -o ConnectTimeout=5 \
              ubuntu@localhost "test -f /tmp/setup-complete && echo READY" 2>/dev/null | grep -q READY; then
              echo "Instance setup completed on attempt $i"
              kill $TUNNEL_PID 2>/dev/null || true
              break
            fi

            kill $TUNNEL_PID 2>/dev/null || true

            if [ $i -eq $MAX_ATTEMPTS ]; then
              echo "Setup timeout after $MAX_ATTEMPTS attempts, continuing anyway..."
              break
            fi

            echo "Attempt $i/$MAX_ATTEMPTS: Setup not complete, waiting 10s..."
            sleep 10
          done

      - name: Create archive of repository
        run: |
          echo "Creating repository archive..."
          git archive --format=tar.gz --output=/tmp/repo.tar.gz HEAD

      - name: Transfer repository to instance
        run: |
          echo "Transferring repository to instance..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          rsync -avz -e "ssh -i /tmp/ec2key -p 2222 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
            /tmp/repo.tar.gz ubuntu@localhost:${{env.WORKDIR}}/

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            ubuntu@localhost \
            "cd ${{env.WORKDIR}} && tar -xzf repo.tar.gz && rm repo.tar.gz"

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Setup Python environment and run warmup
        run: |
          echo "Setting up Python environment and running warmup.py..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -e
          cd ${{env.WORKDIR}}

          # Run warmup.py
          echo "Running warmup.py..."
          python3 warmup.py --skip-confirmation

          echo "Warmup completed"
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Build Rust project
        run: |
          echo "Building Rust project with cargo..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -e
          cd ${{env.WORKDIR}}

          # Setup Rust environment
          source "$HOME/.cargo/env"

          # Build the project
          echo "Running cargo build --release..."
          cargo build --release

          echo "Cargo build completed"
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Setup CI directory
        run: |
          INSTANCE_ID=$(cat /tmp/instance_id.txt)
          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5
          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            ubuntu@localhost "mkdir -p /tmp/ci"
          kill $TUNNEL_PID 2>/dev/null || true

      - name: Run twist
        run: |
          echo "Running twist..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -eo pipefail
          cd ${{env.WORKDIR}}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/twist.ipynb" --output "/tmp/twist_base.py"

          # Create the runnable script with proper imports (using printf to avoid nested heredoc)
          printf '%s\n' 'import sys' 'import os' '' '# Add the repository root to Python path' "sys.path.insert(0, '${{env.WORKDIR}}')" "sys.path.insert(0, '${{env.WORKDIR}}/frontend')" '' '# Set environment variables' "os.environ['"'PYTHONPATH'"'] = '${{env.WORKDIR}}:${{env.WORKDIR}}/frontend:' + os.environ.get('"'PYTHONPATH'"', '')" > /tmp/twist.py

          # Append the converted notebook content
          cat "/tmp/twist_base.py" >> /tmp/twist.py

          # Create output directory for this example
          mkdir -p /tmp/ci/twist

          # Run the example
          echo "twist" > frontend/.CI
          python3 /tmp/twist.py 2>&1 | tee /tmp/ci/twist/twist.log
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Run five-twist
        run: |
          echo "Running five-twist..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -eo pipefail
          cd ${{env.WORKDIR}}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/five-twist.ipynb" --output "/tmp/five-twist_base.py"

          # Create the runnable script with proper imports (using printf to avoid nested heredoc)
          printf '%s\n' 'import sys' 'import os' '' '# Add the repository root to Python path' "sys.path.insert(0, '${{env.WORKDIR}}')" "sys.path.insert(0, '${{env.WORKDIR}}/frontend')" '' '# Set environment variables' "os.environ['"'PYTHONPATH'"'] = '${{env.WORKDIR}}:${{env.WORKDIR}}/frontend:' + os.environ.get('"'PYTHONPATH'"', '')" > /tmp/five-twist.py

          # Append the converted notebook content
          cat "/tmp/five-twist_base.py" >> /tmp/five-twist.py

          # Create output directory for this example
          mkdir -p /tmp/ci/five-twist

          # Run the example
          echo "five-twist" > frontend/.CI
          python3 /tmp/five-twist.py 2>&1 | tee /tmp/ci/five-twist/five-twist.log
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Run woven
        run: |
          echo "Running woven..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -eo pipefail
          cd ${{env.WORKDIR}}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/woven.ipynb" --output "/tmp/woven_base.py"

          # Create the runnable script with proper imports (using printf to avoid nested heredoc)
          printf '%s\n' 'import sys' 'import os' '' '# Add the repository root to Python path' "sys.path.insert(0, '${{env.WORKDIR}}')" "sys.path.insert(0, '${{env.WORKDIR}}/frontend')" '' '# Set environment variables' "os.environ['"'PYTHONPATH'"'] = '${{env.WORKDIR}}:${{env.WORKDIR}}/frontend:' + os.environ.get('"'PYTHONPATH'"', '')" > /tmp/woven.py

          # Append the converted notebook content
          cat "/tmp/woven_base.py" >> /tmp/woven.py

          # Create output directory for this example
          mkdir -p /tmp/ci/woven

          # Run the example
          echo "woven" > frontend/.CI
          python3 /tmp/woven.py 2>&1 | tee /tmp/ci/woven/woven.log
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true

      - name: Run yarn
        run: |
          echo "Running yarn..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          aws ec2-instance-connect open-tunnel --instance-id "$INSTANCE_ID" --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=10 \
            ubuntu@localhost << 'ENDSSH'
          set -eo pipefail
          cd ${{env.WORKDIR}}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/yarn.ipynb" --output "/tmp/yarn_base.py"

          # Create the runnable script with proper imports (using printf to avoid nested heredoc)
          printf '%s\n' 'import sys' 'import os' '' '# Add the repository root to Python path' "sys.path.insert(0, '${{env.WORKDIR}}')" "sys.path.insert(0, '${{env.WORKDIR}}/frontend')" '' '# Set environment variables' "os.environ['"'PYTHONPATH'"'] = '${{env.WORKDIR}}:${{env.WORKDIR}}/frontend:' + os.environ.get('"'PYTHONPATH'"', '')" > /tmp/yarn.py

          # Append the converted notebook content
          cat "/tmp/yarn_base.py" >> /tmp/yarn.py

          # Create output directory for this example
          mkdir -p /tmp/ci/yarn

          # Run the example
          echo "yarn" > frontend/.CI
          python3 /tmp/yarn.py 2>&1 | tee /tmp/ci/yarn/yarn.log
          ENDSSH

          kill $TUNNEL_PID 2>/dev/null || true


      - name: Collect results
        if: success() || failure()
        run: |
          echo "Collecting results from all runs..."
          mkdir -p ci

          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          # Open tunnel for this step
          aws ec2-instance-connect open-tunnel \
            --instance-id "$INSTANCE_ID" \
            --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          # Delete large binary files on remote before copying to save bandwidth
          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            ubuntu@localhost \
            "find ~/.cache/ppf-cts/ci -type f \( -name '*.bin' -o -name '*.pickle' -o -name '*.ply' -o -name '*.gz' \) -delete 2>/dev/null" || true

          # Copy CI output from ppf-cts cache directory
          rsync -avz -e "ssh -i /tmp/ec2key -p 2222 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
            ubuntu@localhost:~/.cache/ppf-cts/ci/ ./ci/ || echo "No ppf-cts CI files found"

          # Also copy logs from /tmp/ci
          rsync -avz -e "ssh -i /tmp/ec2key -p 2222 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
            ubuntu@localhost:/tmp/ci/ ./ci/ || echo "No log files found"

          echo "## Collected Files:"
          ls -laR ci/ | head -100

          # Close tunnel
          kill $TUNNEL_PID 2>/dev/null || true

      - name: Upload artifact
        if: success() || failure()
        uses: actions/upload-artifact@v4
        with:
          name: ci-batch-5
          path: ci
          retention-days: 3

      - name: GPU information
        if: success() || failure()
        run: |
          echo "Getting GPU information..."
          INSTANCE_ID=$(cat /tmp/instance_id.txt)

          # Open tunnel for this step
          aws ec2-instance-connect open-tunnel \
            --instance-id "$INSTANCE_ID" \
            --local-port 2222 &
          TUNNEL_PID=$!
          sleep 5

          ssh -i /tmp/ec2key -p 2222 \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            ubuntu@localhost "nvidia-smi" || echo "Failed to get GPU info"

          # Close tunnel
          kill $TUNNEL_PID 2>/dev/null || true

      - name: Re-authenticate for cleanup
        if: always()
        continue-on-error: true
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-duration-seconds: 21600

      - name: Cleanup - Terminate Instance
        if: always()
        continue-on-error: true
        run: |
          if [ -n "${{ steps.instance.outputs.INSTANCE_ID }}" ]; then
            echo "Initiating instance termination: ${{ steps.instance.outputs.INSTANCE_ID }}"
            aws ec2 terminate-instances \
              --instance-ids "${{ steps.instance.outputs.INSTANCE_ID }}" \
              --region "$AWS_REGION" || true
            echo "Termination initiated. Instance will terminate in the background."
          else
            echo "No instance to terminate"
          fi

      - name: Summary
        if: always()
        run: |
          echo "## Workflow Summary - Batch 5"
          echo "- Region: $AWS_REGION"
          echo "- Instance Type: $INSTANCE_TYPE"
          echo "- Branch: $BRANCH"
          echo "- Examples: $EXAMPLES"
          echo "- Instance ID: ${{ steps.instance.outputs.INSTANCE_ID || 'Not launched' }}"

